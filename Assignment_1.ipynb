{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m_R4z-lmbSjo"
   },
   "source": [
    "<h2>Assignment 1 - Linear Regression on Boston Housing Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJ3cJxt3bSjp"
   },
   "outputs": [],
   "source": [
    "# The modules we're going to use\n",
    "from __future__ import print_function\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import linalg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# When you execute a code to plot with a simple SHIFT-ENTER, the plot will be shown directly under the code cell\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXXIgKFJbSjr"
   },
   "outputs": [],
   "source": [
    "# Load data from scikit-learn, which returns (data, target)\n",
    "# note: if you call \"boston = load_boston()\", it returns a dictionary-like object\n",
    "data, target = datasets.load_boston(True)\n",
    "\n",
    "# Split the data into two parts: training data and testing data\n",
    "train_data,test_data,train_target,test_target = train_test_split(data,(target[:, np.newaxis]), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NA3ezrA-bSjt"
   },
   "source": [
    "<h4>Use scikit-learn library in the following cell</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hlqomvfpbSjt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0 : 30.247\n",
      "W1 : -0.11306\n",
      "W2 : 0.03011\n",
      "W3 : 0.04038\n",
      "W4 : 2.78444\n",
      "W5 : -17.20263\n",
      "W6 : 4.43884\n",
      "W7 : -0.00630\n",
      "W8 : -1.44787\n",
      "W9 : 0.26243\n",
      "W10: -0.01065\n",
      "W11: -0.91546\n",
      "W12: 0.01235\n",
      "W13: -0.50857\n",
      "Training Set MSE: 21.64141\n",
      "Test Set MSE: 24.29112\n"
     ]
    }
   ],
   "source": [
    "# Task 1-1: use linear regression in sklearn\n",
    "model = linear_model.LinearRegression().fit(train_data, train_target)\n",
    "\n",
    "\n",
    "# Task 1-2: show intercept and coefficents\n",
    "print('W0 : {:2.5}'.format(model.intercept_[0]))\n",
    "coefs = model.coef_[0]\n",
    "for i in range(len(coefs)):\n",
    "    print('W{:<2}: {:.5f}'.format(i + 1, coefs[i]))\n",
    "\n",
    "# Task 1-3: show errors on training dataset and testing dataset\n",
    "train_predict = model.predict(train_data)\n",
    "test_predict = model.predict(test_data)\n",
    "train_error = mean_squared_error(train_target, train_predict)\n",
    "test_error = mean_squared_error(test_target, test_predict)\n",
    "\n",
    "print('Training Set MSE: {:.5f}'.format(train_error))\n",
    "print('Test Set MSE: {:.5f}'.format(test_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RpX2tOa5bSjv"
   },
   "source": [
    "<h4>Use analytical solution (normal equation) to perform linear regression in the following cell</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZJJ0sUGbSjv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0 : 30.24675\n",
      "W1 : -0.11306\n",
      "W2 : 0.03011\n",
      "W3 : 0.04038\n",
      "W4 : 2.78444\n",
      "W5 : -17.20263\n",
      "W6 : 4.43884\n",
      "W7 : -0.00630\n",
      "W8 : -1.44787\n",
      "W9 : 0.26243\n",
      "W10: -0.01065\n",
      "W11: -0.91546\n",
      "W12: 0.01235\n",
      "W13: -0.50857\n",
      "Training Set MSE: 21.64141\n",
      "Test Set MSE: 24.29112\n"
     ]
    }
   ],
   "source": [
    "# Task 2-1: Implement a function solving normal equation \n",
    "# Inputs: Training data and  training label\n",
    "# Output: Weights\n",
    "def myNormalEqualFun(X,y):\n",
    "    return np.matmul(np.matmul(linalg.inv(np.matmul(np.transpose(X), X)), np.transpose(X)), y)  \n",
    "    \n",
    "# Task 2-2: Implement a function performing prediction\n",
    "# Inputs: Testing data and weights\n",
    "# Output: Predictions\n",
    "def myPredictFun(X,w):\n",
    "    return np.matmul(X, w)\n",
    "\n",
    "# Here we insert a column of 1s into training_data and test_data (to be consistent with our lecture slides)\n",
    "train_data_intercept = np.insert(train_data, 0, 1, axis=1)\n",
    "test_data_intercept = np.insert(test_data, 0, 1, axis=1)\n",
    "\n",
    "# Here we call myNormalEqual to train the model and get weights\n",
    "w = myNormalEqualFun(train_data_intercept,train_target)\n",
    "\n",
    "# Task 2-3: show intercept and coefficents\n",
    "for i in range(len(w)):\n",
    "    print(\"W{:<2}: {:.5f}\".format(i, w[i][0]))\n",
    "\n",
    "# Task 2-4: show errors on training dataset and testing dataset\n",
    "train_predict = myPredictFun(train_data_intercept, w)\n",
    "test_predict = myPredictFun(test_data_intercept, w)\n",
    "train_error = mean_squared_error(train_predict, train_target)\n",
    "test_error = mean_squared_error(test_predict, test_target)\n",
    "\n",
    "print('Training Set MSE: {:.5f}'.format(train_error))\n",
    "print('Test Set MSE: {:.5f}'.format(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hhVEfixbSjx"
   },
   "source": [
    "<h4>Use numerical solution (baisc gradient descent) to perform linear regression in the following cell</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3H1IxOBubSjy"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-208d235fb03f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Here we call myGradientDescentFun to train the model and get weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Note: you need to figure out good learning rate value and the number of iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyGradientDescentFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_intercept\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Task 3-3: show intercept and coefficents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Feature scaling\n",
    "scaler = preprocessing.StandardScaler().fit(train_data)\n",
    "train_data = scaler.transform(train_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "\n",
    "\n",
    "# Task 3-1: Implement a function performing gradient descent\n",
    "# Inputs: Training data, training label, leaerning rate, number of iterations\n",
    "# Output: the final Weights\n",
    "#         the loss history along iterations\n",
    "def myGradientDescentFun(X,y,learning_rate,numItrs):\n",
    "    return \n",
    "\n",
    "# Task 3-2: Implement a function performing prediction\n",
    "# Inputs: Testing data and weights\n",
    "# Output: Predictions\n",
    "def myPredictFun(X,w):\n",
    "    return\n",
    "\n",
    "# Here we insert a column of 1s into training_data and test_data (to be consistent with our lecture slides)\n",
    "train_data_intercept = np.insert(train_data, 0, 1, axis=1)\n",
    "test_data_intercept = np.insert(test_data, 0, 1, axis=1)\n",
    "\n",
    "# Here we call myGradientDescentFun to train the model and get weights\n",
    "# Note: you need to figure out good learning rate value and the number of iterations\n",
    "w, loss = myGradientDescentFun(train_data_intercept,train_target,0,0)\n",
    "\n",
    "# Task 3-3: show intercept and coefficents\n",
    "\n",
    "\n",
    "# Task 3-4: show errors on training dataset and testing dataset\n",
    "\n",
    "\n",
    "# Task 3-5: plot learning curves showing training errors and testing errors along iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rx1UOqmfcNe0"
   },
   "source": [
    "<h4>Use numerical solution (stochastic gradient descent) to perform linear regression in the following cell</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96_WVyVKcNss"
   },
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = preprocessing.StandardScaler().fit(train_data)\n",
    "train_data = scaler.transform(train_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "\n",
    "\n",
    "# Task 4-1: Implement a function performing gradient descent\n",
    "# Inputs: Training data, training label, leaerning rate, number of epoches, batch size\n",
    "# Output: the final Weights\n",
    "#         the loss history along batches\n",
    "def myGradientDescentFun(X,y,learning_rate,epoches, batchsize):\n",
    "    return \n",
    "\n",
    "# Task 4-2: Implement a function performing prediction\n",
    "# Inputs: Testing data and weights\n",
    "# Output: Predictions\n",
    "def myPredictFun(X,w):\n",
    "    return\n",
    "\n",
    "# Here we insert a column of 1s into training_data and test_data (to be consistent with our lecture slides)\n",
    "train_data_intercept = np.insert(train_data, 0, 1, axis=1)\n",
    "test_data_intercept = np.insert(test_data, 0, 1, axis=1)\n",
    "\n",
    "# Here we call myGradientDescentFun to train the model and get weights\n",
    "# Note: you need to figure out good learning rate value and the number of iterations\n",
    "w, loss = myGradientDescentFun(train_data_intercept,train_target,0,0,0)\n",
    "\n",
    "# Task 4-3: show intercept and coefficents\n",
    "\n",
    "\n",
    "# Task 4-4: show errors on training dataset and testing dataset\n",
    "\n",
    "\n",
    "# Task 4-5: plot learning curves showing training errors and testing errors along bath"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
